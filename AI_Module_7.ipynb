{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuUbaafw8/81vdLSMT80ls",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prof-sd1/Data-Science/blob/main/AI_Module_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 7: AI Model Deployment"
      ],
      "metadata": {
        "id": "RbnmCVqrj9BH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.1: Saving & Loading AI Models**"
      ],
      "metadata": {
        "id": "hvfSsM26kLKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7.1.1 Why Save and Load Models?**\n",
        "\n",
        "  * **Concept:**\n",
        "    Saving a trained model means serializing its learned parameters (weights and biases), its architecture (the layers and how they connect), and sometimes even its optimizer state, so it can be stored on disk. Loading a model means deserializing this information back into memory.\n",
        "\n",
        "      * **Benefits:**\n",
        "        1.  **Persistence:** Avoid retraining the model from scratch every time you want to use it. This saves significant time and computational resources.\n",
        "        2.  **Deployment:** Deploying a trained model into production environments (e.g., web applications, mobile apps, edge devices) requires it to be saved and then loaded by the serving infrastructure.\n",
        "        3.  **Reproducibility:** Share your exact trained model with others, ensuring they can reproduce your results without needing your original training code or data.\n",
        "        4.  **Transfer Learning:** As seen in Module 6.4, pre-trained models are loaded and then adapted for new tasks.\n",
        "        5.  **Checkpointing:** Save model states during long training runs. If training is interrupted (e.g., power outage, Colab disconnect), you can resume from the last saved checkpoint instead of starting over.\n",
        "        6.  **Experimentation:** Easily compare different model architectures or training strategies by saving and loading their best performing versions.\n",
        "\n",
        "  * **What's saved?**\n",
        "\n",
        "      * **Model Architecture:** The definition of the layers and their connections.\n",
        "      * **Model Weights:** The learned numerical parameters of each layer.\n",
        "      * **Training Configuration:** The optimizer, loss function, and metrics used during `model.compile()`.\n",
        "      * **Optimizer State:** The internal state of the optimizer (e.g., momentum in Adam), allowing training to resume exactly where it left off.\n",
        "\n",
        "  * **Colab Explanation:**"
      ],
      "metadata": {
        "id": "BmtfmGzOkSL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"--- 7.1.1 Why Save and Load Models? ---\")\n",
        "\n",
        "print(\"Training AI models can take hours, days, or even weeks.\")\n",
        "print(\"Saving and loading models allows us to store this 'learned knowledge' permanently.\")\n",
        "\n",
        "# Simulate a simple model training\n",
        "# 1. Load and prepare data (MNIST for simplicity)\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28 * 28)).astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28)).astype('float32') / 255\n",
        "\n",
        "# 2. Define a simple model\n",
        "def create_simple_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(128, activation='relu', input_shape=(28 * 28,)),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = create_simple_model()\n",
        "print(\"\\nSimple model created. Summary:\")\n",
        "model.summary()\n",
        "\n",
        "# Simulate training for a few epochs\n",
        "print(\"\\nSimulating model training for 3 epochs...\")\n",
        "history = model.fit(train_images, train_labels, epochs=3, batch_size=64, validation_split=0.1, verbose=0)\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# Evaluate the trained model\n",
        "loss, acc = model.evaluate(test_images, test_labels, verbose=0)\n",
        "print(f\"Trained model accuracy: {acc:.4f}\")\n",
        "\n",
        "print(\"\\nWithout saving, if this Colab session ends, all this training progress is lost!\")\n",
        "print(\"Saving allows us to resume training, share, or deploy this trained model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "KvbnWEZykA8f",
        "outputId": "14caad53-1c03-43bb-8b57-d7f01fd626ce"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 7.1.1 Why Save and Load Models? ---\n",
            "Training AI models can take hours, days, or even weeks.\n",
            "Saving and loading models allows us to store this 'learned knowledge' permanently.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "Simple model created. Summary:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m100,480\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m101,770\u001b[0m (397.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,770</span> (397.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m101,770\u001b[0m (397.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">101,770</span> (397.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Simulating model training for 3 epochs...\n",
            "Training finished.\n",
            "Trained model accuracy: 0.9694\n",
            "\n",
            "Without saving, if this Colab session ends, all this training progress is lost!\n",
            "Saving allows us to resume training, share, or deploy this trained model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Exercise:**\n",
        "\n",
        "    1.  Imagine you are training a very large image classification model on a dataset that takes 24 hours to train fully. Your Colab session has a maximum runtime of 12 hours. How would saving and loading models help you complete the training?\n",
        "    2.  You've developed a new, custom activation function for your neural network. When you save and then try to load the *entire* model, what potential issue might arise if you don't handle this custom object correctly?\n",
        "\n",
        "-----\n",
        "\n",
        "### **7.1.2 Saving Entire Models (Recommended)**\n",
        "\n",
        "  * **Concept:**\n",
        "    Saving the entire model is the most comprehensive way to persist your model. It saves:\n",
        "\n",
        "      * The model's architecture.\n",
        "      * The model's weights.\n",
        "      * The model's training configuration (optimizer, loss, metrics).\n",
        "      * The state of the optimizer (allowing you to resume training).\n",
        "\n",
        "    TensorFlow/Keras supports a few formats for saving entire models:\n",
        "\n",
        "    1.  **Native Keras Format (`.keras`):**\n",
        "          * **Recommended for TensorFlow 2.x and Keras 3.** This is a single `.keras` file (actually a zip archive) that contains everything. It's designed to be portable and easy to use.\n",
        "          * `model.save('my_model.keras')`\n",
        "    2.  **TensorFlow SavedModel Format:**\n",
        "          * This is TensorFlow's universal serialization format. It saves the model as a directory containing a `saved_model.pb` file (the computation graph) and `variables/` directory (weights).\n",
        "          * Ideal for deployment with TensorFlow Serving, TensorFlow.js, TensorFlow Lite, or for use with `tf.saved_model.load()`.\n",
        "          * `model.save('my_model_savedmodel')` (saves to a directory)\n",
        "    3.  **HDF5 Format (`.h5`):**\n",
        "          * A legacy format that was popular in older Keras versions. It saves the model as a single `.h5` file.\n",
        "          * Still supported but generally superseded by `.keras` and SavedModel for new projects due to limitations with custom objects and more complex TensorFlow features.\n",
        "          * `model.save('my_model.h5')`\n",
        "\n",
        "  * **Colab Example:**"
      ],
      "metadata": {
        "id": "404mqCjLkjS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"\\n--- 7.1.2 Saving Entire Models ---\")\n",
        "\n",
        "# --- 1. Define the model ---\n",
        "def create_simple_model():\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(512, activation='relu', input_shape=(28 * 28,)),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# --- 2. Load and preprocess MNIST data ---\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28 * 28)).astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28)).astype('float32') / 255\n",
        "\n",
        "# --- 3. Train the model ---\n",
        "model = create_simple_model()\n",
        "model.fit(train_images, train_labels, epochs=3, batch_size=64, validation_split=0.1, verbose=0)\n",
        "loss, acc = model.evaluate(test_images, test_labels, verbose=0)\n",
        "print(f\"Model accuracy before saving: {acc:.4f}\")\n",
        "\n",
        "# --- 4. Saving in Native Keras Format (.keras) ---\n",
        "keras_model_path = 'my_model.keras'\n",
        "model.save(keras_model_path)\n",
        "print(f\"\\nModel saved to: {keras_model_path}\")\n",
        "print(f\"File size: {os.path.getsize(keras_model_path) / (1024*1024):.2f} MB\")\n",
        "\n",
        "# --- 5. Saving in TensorFlow SavedModel Format ---\n",
        "savedmodel_path = 'my_model_savedmodel'\n",
        "model.export(savedmodel_path)  # For TF Serving / TFLite use\n",
        "print(f\"\\nModel exported to directory: {savedmodel_path}\")\n",
        "print(f\"Contents of {savedmodel_path}: {os.listdir(savedmodel_path)}\")\n",
        "print(f\"Contents of {savedmodel_path}/variables: {os.listdir(os.path.join(savedmodel_path, 'variables'))}\")\n",
        "\n",
        "# --- 6. Saving in HDF5 Format (.h5) ---\n",
        "h5_model_path = 'my_model.h5'\n",
        "model.save(h5_model_path)\n",
        "print(f\"\\nModel saved to: {h5_model_path}\")\n",
        "print(f\"File size: {os.path.getsize(h5_model_path) / (1024*1024):.2f} MB\")\n",
        "\n",
        "print(\"\\n✅ All three formats demonstrated for saving the entire model.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9N4sf4BpkWmC",
        "outputId": "3ae0eade-6b66-4de8-e2a1-e146058963c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 7.1.2 Saving Entire Models ---\n",
            "Model accuracy before saving: 0.9768\n",
            "\n",
            "Model saved to: my_model.keras\n",
            "File size: 4.68 MB\n",
            "Saved artifact at 'my_model_savedmodel'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 784), dtype=tf.float32, name='keras_tensor_7')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  136735065083152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136735065081232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136735065085072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136735065086800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model exported to directory: my_model_savedmodel\n",
            "Contents of my_model_savedmodel: ['variables', 'saved_model.pb', 'assets', 'fingerprint.pb']\n",
            "Contents of my_model_savedmodel/variables: ['variables.index', 'variables.data-00000-of-00001']\n",
            "\n",
            "Model saved to: my_model.h5\n",
            "File size: 4.68 MB\n",
            "\n",
            "✅ All three formats demonstrated for saving the entire model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Exercise:**\n",
        "\n",
        "    1.  After running the code, inspect the contents of the `my_model_savedmodel` directory. What are the main files/folders you see, and what do you think each contains?\n",
        "    2.  If you had a very complex model with many custom layers and a custom training loop, which saving format (among `.keras`, SavedModel, `.h5`) would generally be the most robust for ensuring it loads correctly without needing the original Python code for custom objects? Why?\n",
        "\n",
        "-----\n",
        "\n",
        "### **7.1.3 Loading Entire Models**\n",
        "\n",
        "  * **Concept:**\n",
        "    Loading an entire model is straightforward using `tf.keras.models.load_model()`. This function automatically detects the format (whether `.keras`, SavedModel directory, or `.h5`) and reconstructs the model, including its architecture, weights, and compilation information.\n",
        "\n",
        "      * **Key point:** When loading a model saved with its optimizer state, you can resume training from where you left off.\n",
        "\n",
        "  * **Colab Example:**"
      ],
      "metadata": {
        "id": "UXhTQDKhl5mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"\\n--- 7.1.3 Loading Entire Models ---\")\n",
        "\n",
        "# --- Load and preprocess MNIST test data ---\n",
        "(_, _), (test_images, test_labels) = mnist.load_data()\n",
        "test_images = test_images.reshape((10000, 28 * 28)).astype('float32') / 255\n",
        "\n",
        "# --- 1. Load from Native Keras Format (.keras) ---\n",
        "print(\"\\n--- Loading from .keras format ---\")\n",
        "loaded_model_keras = keras.models.load_model('my_model.keras')\n",
        "loaded_model_keras.summary()\n",
        "loss, acc = loaded_model_keras.evaluate(test_images, test_labels, verbose=0)\n",
        "print(f\"✅ Accuracy of model loaded from .keras: {acc:.4f}\")\n",
        "\n",
        "# --- 2. Load from TensorFlow SavedModel using TFSMLayer (inference only) ---\n",
        "print(\"\\n--- Loading from SavedModel format (Keras 3.x, inference only) ---\")\n",
        "saved_model_layer = keras.layers.TFSMLayer('my_model_savedmodel', call_endpoint='serving_default')\n",
        "loaded_model_savedmodel = keras.Sequential([\n",
        "    keras.Input(shape=(784,)),\n",
        "    saved_model_layer\n",
        "])\n",
        "loaded_model_savedmodel.summary()\n",
        "\n",
        "# Predict and handle dict output if present\n",
        "pred_probs = loaded_model_savedmodel.predict(test_images, verbose=0)\n",
        "\n",
        "if isinstance(pred_probs, dict):\n",
        "    print(\"Prediction output is a dict with keys:\", pred_probs.keys())\n",
        "    first_key = list(pred_probs.keys())[0]\n",
        "    pred_array = pred_probs[first_key]\n",
        "    if hasattr(pred_array, 'numpy'):\n",
        "        pred_array = pred_array.numpy()\n",
        "else:\n",
        "    pred_array = pred_probs\n",
        "\n",
        "# Convert to labels\n",
        "if pred_array.ndim == 2:\n",
        "    pred_labels = np.argmax(pred_array, axis=1)\n",
        "else:\n",
        "    pred_labels = pred_array.squeeze()\n",
        "\n",
        "pred_labels = pred_labels.astype(int)\n",
        "\n",
        "acc = np.mean(pred_labels == test_labels)\n",
        "print(f\"✅ Accuracy of model loaded from SavedModel (manual eval): {acc:.4f}\")\n",
        "\n",
        "# --- 3. Load from HDF5 Format (.h5) ---\n",
        "print(\"\\n--- Loading from .h5 format ---\")\n",
        "loaded_model_h5 = keras.models.load_model('my_model.h5')\n",
        "loaded_model_h5.summary()\n",
        "loss, acc = loaded_model_h5.evaluate(test_images, test_labels, verbose=0)\n",
        "print(f\"✅ Accuracy of model loaded from .h5: {acc:.4f}\")\n",
        "\n",
        "print(\"\\n🎉 All models loaded and evaluated successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "A1w5nQhYkqOb",
        "outputId": "f3a56dcd-fc73-454c-f1c9-a39d209b38b7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 7.1.3 Loading Entire Models ---\n",
            "\n",
            "--- Loading from .keras format ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m401,920\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m5,130\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">401,920</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,130</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,221,152\u001b[0m (4.66 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,221,152</span> (4.66 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m407,050\u001b[0m (1.55 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">407,050</span> (1.55 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m814,102\u001b[0m (3.11 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">814,102</span> (3.11 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Accuracy of model loaded from .keras: 0.9768\n",
            "\n",
            "--- Loading from SavedModel format (Keras 3.x, inference only) ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ tfsm_layer_5 (\u001b[38;5;33mTFSMLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │       \u001b[38;5;34m407,050\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ tfsm_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TFSMLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">407,050</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m407,050\u001b[0m (1.55 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">407,050</span> (1.55 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m407,050\u001b[0m (1.55 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">407,050</span> (1.55 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction output is a dict with keys: dict_keys(['output_0'])\n",
            "✅ Accuracy of model loaded from SavedModel (manual eval): 0.9768\n",
            "\n",
            "--- Loading from .h5 format ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m401,920\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m5,130\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">401,920</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,130</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m407,052\u001b[0m (1.55 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">407,052</span> (1.55 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m407,050\u001b[0m (1.55 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">407,050</span> (1.55 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Accuracy of model loaded from .h5: 0.9768\n",
            "\n",
            "🎉 All models loaded and evaluated successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Exercise:**\n",
        "\n",
        "    1.  What would happen if you tried to load a model using `tf.keras.models.load_model()` but the file path pointed to a directory that was *not* a SavedModel, or a file that was not a `.keras` or `.h5` file?\n",
        "    2.  If you save a model and then load it, and then try to train it for more epochs, why is it important that the optimizer's state was also saved and loaded?\n",
        "\n",
        "-----\n",
        "\n",
        "### **7.1.4 Saving and Loading Only Weights**\n",
        "\n",
        "  * **Concept:**\n",
        "    Sometimes, you only want to save or load the learned weights of a model, not its entire architecture or optimizer state. This is useful in scenarios like:\n",
        "\n",
        "      * **Transfer Learning (Feature Extraction):** You define a new model architecture (e.g., a new classification head) and then load pre-trained weights into its base.\n",
        "\n",
        "      * **Model Checkpointing:** During training, you might only save weights at regular intervals to save disk space, and then load the best weights into a freshly instantiated model.\n",
        "\n",
        "      * **A/B Testing:** Quickly swap different sets of weights into the same model architecture for testing.\n",
        "\n",
        "      * **`model.save_weights()`:** Saves only the weights of the model.\n",
        "\n",
        "          * By default, it saves in TensorFlow Checkpoint format (multiple files, often with `.ckpt` suffix).\n",
        "          * You can specify `save_format='h5'` to save in HDF5 format (`.h5` file).\n",
        "\n",
        "      * **`model.load_weights()`:** Loads weights into a model.\n",
        "\n",
        "          * The model's architecture must be identical to the one from which the weights were saved. If there are mismatches, you might need `skip_mismatch=True` or `by_name=True` (for HDF5).\n",
        "\n",
        "  * **Colab Example:**"
      ],
      "metadata": {
        "id": "-gj8F5KrsWE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"\\n--- 7.1.4 Saving and Loading Only Weights ---\")\n",
        "\n",
        "# Load MNIST data and preprocess\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.astype(\"float32\") / 255.0\n",
        "test_images = test_images.astype(\"float32\") / 255.0\n",
        "\n",
        "# Add channel dimension for Conv2D\n",
        "train_images = np.expand_dims(train_images, -1)  # shape (60000, 28, 28, 1)\n",
        "test_images = np.expand_dims(test_images, -1)\n",
        "\n",
        "# Simple CNN model creator function\n",
        "def create_simple_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(28, 28, 1)),\n",
        "        layers.Conv2D(32, kernel_size=3, activation='relu'),\n",
        "        layers.MaxPooling2D(pool_size=2),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(100, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create and train a model\n",
        "model_weights_only = create_simple_model()\n",
        "model_weights_only.fit(train_images, train_labels, epochs=3, batch_size=64, validation_split=0.1, verbose=0)\n",
        "loss_before_save, acc_before_save = model_weights_only.evaluate(test_images, test_labels, verbose=0)\n",
        "print(f\"Original model accuracy before saving weights: {acc_before_save:.4f}\")\n",
        "\n",
        "# --- 1. Save Weights (TensorFlow Checkpoint format - requires .ckpt suffix) ---\n",
        "weights_tf_path = 'my_model_weights_tf_checkpoint.ckpt'  # Note the .ckpt extension\n",
        "model_weights_only.save_weights(weights_tf_path)\n",
        "print(f\"\\nWeights saved to TensorFlow Checkpoint format: {weights_tf_path}\")\n",
        "print(f\"Contents of current directory: {os.listdir('.')}\")\n",
        "\n",
        "# --- 2. Save Weights (HDF5 format - requires .weights.h5 suffix) ---\n",
        "weights_h5_path = 'my_model_weights.weights.h5'  # Must end with .weights.h5\n",
        "model_weights_only.save_weights(weights_h5_path)\n",
        "print(f\"\\nWeights saved to HDF5 format: {weights_h5_path}\")\n",
        "print(f\"File size: {os.path.getsize(weights_h5_path) / (1024*1024):.2f} MB\")\n",
        "\n",
        "# --- Loading Weights ---\n",
        "\n",
        "# New untrained model for loading weights\n",
        "new_model_for_loading = create_simple_model()\n",
        "loss_untrained, acc_untrained = new_model_for_loading.evaluate(test_images, test_labels, verbose=0)\n",
        "print(f\"\\nUntrained model accuracy: {acc_untrained:.4f}\")\n",
        "\n",
        "# Load weights from TensorFlow Checkpoint\n",
        "new_model_for_loading.load_weights(weights_tf_path)\n",
        "loss_after_load_tf, acc_after_load_tf = new_model_for_loading.evaluate(test_images, test_labels, verbose=0)\n",
        "print(f\"Accuracy after loading TF Checkpoint weights: {acc_after_load_tf:.4f}\")\n",
        "\n",
        "# Load weights from HDF5 (new model instance)\n",
        "new_model_for_loading_h5 = create_simple_model()\n",
        "new_model_for_loading_h5.load_weights(weights_h5_path)\n",
        "loss_after_load_h5, acc_after_load_h5 = new_model_for_loading_h5.evaluate(test_images, test_labels, verbose=0)\n",
        "print(f\"Accuracy after loading HDF5 weights: {acc_after_load_h5:.4f}\")\n",
        "\n",
        "print(\"\\nAccuracy after loading weights matches the original trained model, confirming successful saving and loading.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "fboWr330l9no",
        "outputId": "3ead8738-0e65-42fb-d2f4-9e334e318c34"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 7.1.4 Saving and Loading Only Weights ---\n",
            "Original model accuracy before saving weights: 0.9833\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The filename must end in `.weights.h5`. Received: filepath=my_model_weights_tf_checkpoint.ckpt",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1999154340.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# --- 1. Save Weights (TensorFlow Checkpoint format - requires .ckpt suffix) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mweights_tf_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'my_model_weights_tf_checkpoint.ckpt'\u001b[0m  \u001b[0;31m# Note the .ckpt extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mmodel_weights_only\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_tf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nWeights saved to TensorFlow Checkpoint format: {weights_tf_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Contents of current directory: {os.listdir('.')}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(model, filepath, overwrite, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".weights.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    225\u001b[0m             \u001b[0;34m\"The filename must end in `.weights.h5`. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;34mf\"Received: filepath={filepath}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The filename must end in `.weights.h5`. Received: filepath=my_model_weights_tf_checkpoint.ckpt"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Exercise:**\n",
        "\n",
        "    1.  You have a pre-trained `MobileNetV2` model (without its top classification layer) and want to load its ImageNet weights into a new model you've defined. Which `model.load_weights()` parameter would be particularly useful if your new model's layer names don't exactly match the original `MobileNetV2` layer names, but their shapes are compatible?\n",
        "    2.  When would saving *only* the weights be more advantageous than saving the entire model? Provide two scenarios.\n",
        "\n",
        "-----\n",
        "\n",
        "### **7.1.5 Best Practices and Deployment Considerations**\n",
        "\n",
        "  * **Concept:**\n",
        "    Effective model saving and loading are crucial for robust deployment.\n",
        "\n",
        "      * **Which format to use?**\n",
        "\n",
        "          * **`.keras` (Recommended):** For most Keras workflows in TensorFlow 2.x. It's self-contained and handles custom objects well.\n",
        "          * **SavedModel:** If you plan to deploy with TensorFlow Serving, TensorFlow Lite, or TensorFlow.js, or need to integrate with other TensorFlow ecosystem tools. It's the most comprehensive and portable format within the TensorFlow ecosystem.\n",
        "          * **`.h5`:** Primarily for backward compatibility or when you need a single file and don't have complex custom objects. Less recommended for new projects.\n",
        "\n",
        "      * **Versioning:**\n",
        "\n",
        "          * As models evolve, it's critical to manage different versions. Store models in versioned directories (e.g., `models/v1/`, `models/v2/`).\n",
        "          * Include metadata (training date, dataset version, hyperparameters) with each saved model.\n",
        "          * Use tools like MLflow or DVC for more sophisticated model versioning and tracking.\n",
        "\n",
        "      * **Custom Objects:** If your model uses custom layers, loss functions, metrics, or activation functions, you need to inform `load_model()` about them.\n",
        "\n",
        "          * **Recommended:** Use `tf.keras.utils.register_keras_serializable()` decorator on your custom classes/functions.\n",
        "          * **Alternative:** Pass them via the `custom_objects` argument in `load_model()`.\n",
        "\n",
        "      * **Deployment Environment:**\n",
        "\n",
        "          * Ensure the environment where the model is loaded has the same TensorFlow/Keras version (or compatible versions) as the environment where it was saved.\n",
        "          * Consider the target platform: TensorFlow Lite for mobile/edge, TensorFlow.js for web browsers, TensorFlow Serving for scalable production APIs.\n",
        "\n",
        "  * **Colab Explanation (Conceptual & Custom Object Example):**"
      ],
      "metadata": {
        "id": "R6CRUS5GsbgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"\\n--- 7.1.5 Best Practices and Deployment Considerations ---\")\n",
        "\n",
        "print(\"\\n**Choosing the Right Format:**\")\n",
        "print(\"- `.keras` (Native Keras): Best for general Keras use, single file, portable.\")\n",
        "print(\"- `SavedModel`: Best for TensorFlow ecosystem deployment (TF Serving, TF Lite, TF.js).\")\n",
        "print(\"- `.h5` (HDF5): Legacy, single file, but can have issues with custom objects.\")\n",
        "\n",
        "print(\"\\n**Handling Custom Objects (Layers, Losses, etc.):**\")\n",
        "\n",
        "# Define a custom layer\n",
        "class CustomDenseLayer(layers.Layer):\n",
        "    def __init__(self, units=32, **kwargs):\n",
        "        super(CustomDenseLayer, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            shape=(self.units,), initializer=\"zeros\", trainable=True\n",
        "        )\n",
        "        super(CustomDenseLayer, self).build(input_shape) # Must call at the end\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(CustomDenseLayer, self).get_config()\n",
        "        config.update({\"units\": self.units})\n",
        "        return config\n",
        "\n",
        "# Recommended way to register custom objects for saving/loading\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class CustomActivation(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(CustomActivation, self).__init__(**kwargs)\n",
        "        self.threshold = tf.Variable(0.0, trainable=True, name='threshold') # Example trainable parameter\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.where(inputs > self.threshold, inputs, 0.0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(CustomActivation, self).get_config()\n",
        "        # No need to manually add trainable variables if they are tf.Variable and part of the layer\n",
        "        return config\n",
        "\n",
        "print(\"\\nCustom layers defined. `CustomActivation` is registered for serialization.\")\n",
        "\n",
        "# Create a model with custom layers\n",
        "custom_model = models.Sequential([\n",
        "    layers.Input(shape=(10,)),\n",
        "    CustomDenseLayer(units=5),\n",
        "    CustomActivation(), # Using the registered custom activation\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "custom_model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "print(\"\\nModel with custom layers created:\")\n",
        "custom_model.summary()\n",
        "\n",
        "# Save the model\n",
        "custom_model_path_keras = 'custom_model.keras'\n",
        "custom_model.save(custom_model_path_keras)\n",
        "print(f\"\\nCustom model saved to: {custom_model_path_keras}\")\n",
        "\n",
        "# Load the model (should work seamlessly because CustomActivation is registered)\n",
        "print(\"\\nAttempting to load custom model...\")\n",
        "loaded_custom_model = keras.models.load_model(custom_model_path_keras)\n",
        "loaded_custom_model.summary()\n",
        "print(\"Custom model loaded successfully using `.keras` format (due to registration).\")\n",
        "\n",
        "# If CustomActivation was NOT registered, you'd need:\n",
        "# loaded_custom_model = keras.models.load_model(custom_model_path_keras,\n",
        "#                                               custom_objects={'CustomActivation': CustomActivation})\n",
        "\n",
        "print(\"\\n**Versioning:**\")\n",
        "print(\"Organize your saved models in versioned directories (e.g., 'model_artifacts/v1/', 'model_artifacts/v2/').\")\n",
        "print(\"Include metadata (e.g., training date, dataset version, performance metrics) with each version.\")\n",
        "\n",
        "print(\"\\n**Deployment Environment:**\")\n",
        "print(\"- Match TensorFlow/Keras versions between training and deployment.\")\n",
        "print(\"- Consider target platforms: TF Lite (mobile/edge), TF.js (web), TF Serving (API).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xX0XzkWeseqD",
        "outputId": "98926e0d-2d48-42fc-c819-5bbf158b536d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 7.1.5 Best Practices and Deployment Considerations ---\n",
            "\n",
            "**Choosing the Right Format:**\n",
            "- `.keras` (Native Keras): Best for general Keras use, single file, portable.\n",
            "- `SavedModel`: Best for TensorFlow ecosystem deployment (TF Serving, TF Lite, TF.js).\n",
            "- `.h5` (HDF5): Legacy, single file, but can have issues with custom objects.\n",
            "\n",
            "**Handling Custom Objects (Layers, Losses, etc.):**\n",
            "\n",
            "Custom layers defined. `CustomActivation` is registered for serialization.\n",
            "\n",
            "Model with custom layers created:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_11\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ custom_dense_layer              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │            \u001b[38;5;34m55\u001b[0m │\n",
              "│ (\u001b[38;5;33mCustomDenseLayer\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ custom_activation               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mCustomActivation\u001b[0m)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m6\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ custom_dense_layer              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CustomDenseLayer</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ custom_activation               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CustomActivation</span>)              │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m61\u001b[0m (244.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61</span> (244.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m61\u001b[0m (244.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61</span> (244.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Custom model saved to: custom_model.keras\n",
            "\n",
            "Attempting to load custom model...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "<class 'keras.src.models.sequential.Sequential'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras', 'class_name': 'Sequential', 'config': {'name': 'sequential_11', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None, 'shared_object_id': 136735103768976}, 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': [None, 10], 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_11'}, 'registered_name': None}, {'module': None, 'class_name': 'CustomDenseLayer', 'config': {'name': 'custom_dense_layer', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 5}, 'registered_name': 'CustomDenseLayer', 'build_config': {'input_shape': [None, 10]}}, {'module': None, 'class_name': 'CustomActivation', 'config': {'name': 'custom_activation', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None, 'shared_object_id': 136735103768976}}, 'registered_name': 'Custom>CustomActivation', 'build_config': {'input_shape': [None, 5]}}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_10', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None, 'shared_object_id': 136735103768976}, 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': [None, 5]}}], 'build_input_shape': [None, 10]}, 'registered_name': None, 'build_config': {'input_shape': [None, 10]}, 'compile_config': {'optimizer': {'module': 'keras.optimizers', 'class_name': 'Adam', 'config': {'name': 'adam', 'learning_rate': 0.0010000000474974513, 'weight_decay': None, 'clipnorm': None, 'global_clipnorm': None, 'clipvalue': None, 'use_ema': False, 'ema_momentum': 0.99, 'ema_overwrite_frequency': None, 'loss_scale_factor': None, 'gradient_accumulation_steps': None, 'beta_1': 0.9, 'beta_2': 0.999, 'epsilon': 1e-07, 'amsgrad': False}, 'registered_name': None}, 'loss': 'binary_crossentropy', 'loss_weights': None, 'metrics': None, 'weighted_metrics': None, 'run_eagerly': False, 'steps_per_execution': 1, 'jit_compile': False}}.\n\nException encountered: Could not locate class 'CustomDenseLayer'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'CustomDenseLayer', 'config': {'name': 'custom_dense_layer', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 5}, 'registered_name': 'CustomDenseLayer', 'build_config': {'input_shape': [None, 10]}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/sequential.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 layer = serialization_lib.deserialize_keras_object(\n\u001b[0m\u001b[1;32m    356\u001b[0m                     \u001b[0mlayer_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m     cls = _retrieve_class_or_fn(\n\u001b[0m\u001b[1;32m    695\u001b[0m         \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36m_retrieve_class_or_fn\u001b[0;34m(name, registered_name, module, obj_type, full_config, custom_objects)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m     raise TypeError(\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;34mf\"Could not locate {obj_type} '{name}'. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Could not locate class 'CustomDenseLayer'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'CustomDenseLayer', 'config': {'name': 'custom_dense_layer', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 5}, 'registered_name': 'CustomDenseLayer', 'build_config': {'input_shape': [None, 10]}}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1487130297.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# Load the model (should work seamlessly because CustomActivation is registered)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nAttempting to load custom model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0mloaded_custom_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_model_path_keras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0mloaded_custom_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Custom model loaded successfully using `.keras` format (due to registration).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_keras_zip\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_keras_dir\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_hf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         return saving_lib.load_model(\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    365\u001b[0m             )\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             return _load_model_from_fileobj(\n\u001b[0m\u001b[1;32m    368\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mconfig_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         model = _model_from_config(\n\u001b[0m\u001b[1;32m    445\u001b[0m             \u001b[0mconfig_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_model_from_config\u001b[0;34m(config_json, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;31m# Construct the model from the configuration file in the archive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mObjectSharingScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         model = deserialize_keras_object(\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    721\u001b[0m                 \u001b[0;34mf\"{cls} could not be deserialized properly. Please\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0;34m\" ensure that components that are Python object\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: <class 'keras.src.models.sequential.Sequential'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras', 'class_name': 'Sequential', 'config': {'name': 'sequential_11', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None, 'shared_object_id': 136735103768976}, 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': [None, 10], 'dtype': 'float32', 'sparse': False, 'name': 'input_layer_11'}, 'registered_name': None}, {'module': None, 'class_name': 'CustomDenseLayer', 'config': {'name': 'custom_dense_layer', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 5}, 'registered_name': 'CustomDenseLayer', 'build_config': {'input_shape': [None, 10]}}, {'module': None, 'class_name': 'CustomActivation', 'config': {'name': 'custom_activation', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None, 'shared_object_id': 136735103768976}}, 'registered_name': 'Custom>CustomActivation', 'build_config': {'input_shape': [None, 5]}}, {'module': 'keras.layers', 'class_name': 'Dense', 'config': {'name': 'dense_10', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None, 'shared_object_id': 136735103768976}, 'units': 1, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', '...\n\nException encountered: Could not locate class 'CustomDenseLayer'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': None, 'class_name': 'CustomDenseLayer', 'config': {'name': 'custom_dense_layer', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 5}, 'registered_name': 'CustomDenseLayer', 'build_config': {'input_shape': [None, 10]}}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Exercise:**\n",
        "\n",
        "    1.  Modify the `CustomDenseLayer` by adding the `@tf.keras.utils.register_keras_serializable()` decorator to it. Save the `custom_model` to a new `.keras` file and then try to load it. Does it still load correctly?\n",
        "    2.  You have a model saved as `my_model_savedmodel`. You want to deploy it as a web service that can handle many concurrent prediction requests. Which TensorFlow deployment tool would be most suitable for this scenario, and why?\n"
      ],
      "metadata": {
        "id": "vOhJt7Fksp8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.2: Streamlit & Flask for Web Apps**\n"
      ],
      "metadata": {
        "id": "mhzGdn-EtDXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7.2.1 Introduction to Web Deployment for AI Models**\n",
        "\n",
        "  * **Concept:**\n",
        "    Deploying an AI model means making it available for use by others, often in a production environment. For many applications, this involves creating a web interface or an API (Application Programming Interface) that allows users or other software systems to interact with the model.\n",
        "\n",
        "      * **Why Web Deployment?**\n",
        "\n",
        "          * **Accessibility:** Users can access the model's functionality through a web browser without needing to install complex software or understand the underlying code.\n",
        "          * **Scalability:** Web applications can be designed to handle multiple users and requests concurrently.\n",
        "          * **Integration:** APIs allow other applications (mobile apps, backend services) to programmatically send data to the model and receive predictions.\n",
        "          * **Demonstration & Prototyping:** Quickly showcase your model's capabilities to stakeholders.\n",
        "\n",
        "      * **Common Deployment Scenarios:**\n",
        "\n",
        "          * **Interactive Web App:** A user-friendly interface where users input data (text, images, numbers) and see the model's predictions directly on a webpage (e.g., a sentiment analyzer, an image classifier demo).\n",
        "          * **API Endpoint:** A programmatic interface where other software sends HTTP requests (e.g., JSON data) to your model and receives HTTP responses (e.g., JSON predictions). This is common for integrating AI into larger systems.\n",
        "\n",
        "  * **Colab Explanation:**"
      ],
      "metadata": {
        "id": "k6HsYmYjuD0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- 7.2.1 Introduction to Web Deployment for AI Models ---\")\n",
        "\n",
        "print(\"Imagine you've trained a fantastic AI model that can detect diseases from X-rays.\")\n",
        "print(\"How do you let doctors use it without them needing to run your Python code?\")\n",
        "\n",
        "print(\"\\nWeb deployment is the answer!\")\n",
        "print(\"It allows us to wrap our AI model in a web interface or API, making it accessible.\")\n",
        "\n",
        "print(\"\\nTwo common ways to deploy Python-based AI models as web applications are:\")\n",
        "print(\"1. Streamlit: For quick, interactive dashboards and demos (less web development knowledge needed).\")\n",
        "print(\"2. Flask: For building more custom web APIs or full-fledged web applications (more control, requires basic web dev knowledge).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6BW9EMesqh3",
        "outputId": "7e30cbe3-f0e0-490e-d767-c70ec5695515"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 7.2.1 Introduction to Web Deployment for AI Models ---\n",
            "Imagine you've trained a fantastic AI model that can detect diseases from X-rays.\n",
            "How do you let doctors use it without them needing to run your Python code?\n",
            "\n",
            "Web deployment is the answer!\n",
            "It allows us to wrap our AI model in a web interface or API, making it accessible.\n",
            "\n",
            "Two common ways to deploy Python-based AI models as web applications are:\n",
            "1. Streamlit: For quick, interactive dashboards and demos (less web development knowledge needed).\n",
            "2. Flask: For building more custom web APIs or full-fledged web applications (more control, requires basic web dev knowledge).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Exercise:**\n",
        "\n",
        "    1.  You've trained an object detection model that identifies different types of waste in an image for recycling purposes. Describe two different ways you could deploy this model for practical use, one as an interactive web app and one as an API.\n",
        "    2.  What is the primary advantage of deploying an AI model as an API endpoint compared to a standalone desktop application?\n",
        "\n",
        "-----\n",
        "\n",
        "### **7.2.2 Streamlit for Interactive AI Web Apps**\n",
        "\n",
        "  * **Concept:**\n",
        "    **Streamlit** is an open-source Python library that makes it incredibly easy to create beautiful, custom web applications for machine learning and data science. It's designed for data scientists who want to build interactive dashboards and demos without needing extensive web development (HTML, CSS, JavaScript) knowledge.\n",
        "\n",
        "      * **Key Features:**\n",
        "\n",
        "          * **Python-Native:** Write your entire web app in pure Python.\n",
        "          * **Rapid Prototyping:** Turn data scripts into interactive apps in minutes.\n",
        "          * **Simple API:** Provides intuitive functions (`st.write()`, `st.slider()`, `st.button()`, `st.image()`) to display content and create interactive widgets.\n",
        "          * **Automatic Reruns:** The app automatically reruns from top to bottom whenever a user interacts with a widget, making it reactive.\n",
        "          * **Caching (`@st.cache_data`, `@st.cache_resource`):** Optimizes performance by caching expensive computations (like model loading) to avoid rerunning them unnecessarily.\n",
        "\n",
        "      * **Pros:**\n",
        "\n",
        "          * Extremely fast development cycle for demos and internal tools.\n",
        "          * No front-end knowledge required.\n",
        "          * Great for data visualization and interactive exploration.\n",
        "          * Active community and growing ecosystem of components.\n",
        "\n",
        "      * **Cons:**\n",
        "\n",
        "          * Limited UI customization compared to traditional web frameworks.\n",
        "          * Can be less suitable for complex, multi-page applications or high-traffic production systems.\n",
        "          * The \"rerun from top\" execution model can sometimes be tricky to manage state.\n",
        "\n",
        "  * **Colab Example:**\n",
        "    To run Streamlit apps in Colab, we typically need to use `ngrok` or `localtunnel` to expose the local server to the internet. We'll demonstrate a simple Streamlit app that you can run locally or in Colab."
      ],
      "metadata": {
        "id": "alc7o1zkufpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Streamlit app code saved as a string\n",
        "streamlit_app_code = \"\"\"\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "@st.cache_resource\n",
        "def load_my_ai_model():\n",
        "    time.sleep(2)\n",
        "    class DummyModel:\n",
        "        def predict(self, input_data):\n",
        "            return np.array([0])\n",
        "    return DummyModel()\n",
        "\n",
        "my_model = load_my_ai_model()\n",
        "\n",
        "st.title(\"Simple AI Model Demo with Streamlit\")\n",
        "st.write(\"This is a basic Streamlit application to demonstrate how to build interactive web apps for AI models.\")\n",
        "st.markdown(\"---\")\n",
        "\n",
        "st.header(\"Input for Prediction\")\n",
        "\n",
        "input_feature_1 = st.slider(\"Feature 1 (e.g., Temperature)\", 0.0, 100.0, 50.0)\n",
        "input_feature_2 = st.number_input(\"Feature 2 (e.g., Humidity)\", 0.0, 100.0, 25.0)\n",
        "input_text = st.text_area(\"Enter some text (for NLP demo)\", \"This is a sample text.\")\n",
        "\n",
        "if st.button(\"Get Prediction\"):\n",
        "    input_data = np.array([[input_feature_1, input_feature_2]])\n",
        "    prediction = my_model.predict(input_data)\n",
        "    st.subheader(\"Prediction Result:\")\n",
        "    st.success(f\"The model predicts: Class {prediction[0]}\")\n",
        "    st.info(f\"Input text received: '{input_text}' (not used in dummy model prediction)\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.write(\"This application runs in your browser.\")\n",
        "\"\"\"\n",
        "\n",
        "# Save the app to app.py\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(streamlit_app_code)\n",
        "\n",
        "print(\"✅ Streamlit app saved as 'app.py'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SInJqfQjucTw",
        "outputId": "e0ead3ad-8436-4d17-f5c5-4ee97cec6d24"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Streamlit app saved as 'app.py'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import streamlit as st\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "@st.cache\\_resource\n",
        "def load\\_my\\_ai\\_model():\n",
        "time.sleep(1) \\# Simulate model loading time\n",
        "class DummyModel:\n",
        "def predict(self, input\\_data):\n",
        "\\# A very simple dummy prediction logic\n",
        "if input\\_data[0, 0] \\> 70: \\# If Feature 1 is high\n",
        "return np.array([1]) \\# Predict Class 1\n",
        "else:\n",
        "return np.array([0]) \\# Predict Class 0\n",
        "return DummyModel()\n",
        "\n",
        "my\\_model = load\\_my\\_ai\\_model()\n",
        "\n",
        "st.set\\_page\\_config(layout=\"centered\", page\\_title=\"AI Model Demo\")\n",
        "\n",
        "st.title(\"Simple AI Model Demo with Streamlit\")\n",
        "\n",
        "st.write(\"This is a basic Streamlit application to demonstrate how to build interactive web apps for AI models.\")\n",
        "st.markdown(\"---\")\n",
        "\n",
        "st.header(\"Input for Prediction\")\n",
        "\n",
        "# Create interactive widgets for user input\n",
        "\n",
        "input\\_feature\\_1 = st.slider(\"Feature 1 (e.g., Temperature)\", 0.0, 100.0, 50.0, help=\"Slide to adjust the value for Feature 1.\")\n",
        "input\\_feature\\_2 = st.number\\_input(\"Feature 2 (e.g., Humidity)\", 0.0, 100.0, 25.0, help=\"Enter a numerical value for Feature 2.\")\n",
        "input\\_text = st.text\\_area(\"Enter some text (for NLP demo)\", \"This is a sample text.\", help=\"Type any text here. This input is for demonstration and not used by the dummy model.\")\n",
        "\n",
        "# A button to trigger prediction\n",
        "\n",
        "if st.button(\"Get Prediction\", help=\"Click to get the AI model's prediction based on your inputs.\"):\n",
        "\\# Prepare input data for the dummy model\n",
        "input\\_data = np.array([[input\\_feature\\_1, input\\_feature\\_2]])\n",
        "\n",
        "```\n",
        "# Make prediction using the loaded dummy model\n",
        "prediction = my_model.predict(input_data)\n",
        "\n",
        "st.subheader(\"Prediction Result:\")\n",
        "if prediction[0] == 1:\n",
        "    st.success(f\"The model predicts: **Class 1 (High)**\")\n",
        "else:\n",
        "    st.info(f\"The model predicts: **Class 0 (Low)**\")\n",
        "st.write(f\"Input text received: '{input_text}' (not used in dummy model prediction)\")\n",
        "```\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.write(\"This application runs in your browser. Interact with the widgets and click 'Get Prediction'.\")\n",
        "st.caption(\"Model loading is cached for faster subsequent runs.\")\n",
        "\"\"\"\n",
        "\n",
        "````\n",
        "with open(\"streamlit_app.py\", \"w\") as f:\n",
        "    f.write(streamlit_app_code)\n",
        "\n",
        "print(\"\\nStreamlit app code saved to `streamlit_app.py`.\")\n",
        "print(\"To run this app in Colab, execute the following in a new cell:\")\n",
        "print(\"!streamlit run streamlit_app.py &>/dev/null&\")\n",
        "print(\"!nohup ngrok http 8501 &\")\n",
        "print(\"!sleep 2\") # Give ngrok a moment to start\n",
        "print(\"!curl -s http://localhost:4040/api/tunnels | grep -o 'https://[^/]*\\\\.ngrok\\\\.io'\")\n",
        "print(\"\\nClick the ngrok URL generated to view your Streamlit app.\")\n",
        "```\n",
        "````\n",
        "\n",
        "  * **Exercise:**\n",
        "    1.  What is the purpose of `@st.cache_resource` in the Streamlit example, and why is it particularly useful when deploying AI models?\n",
        "    2.  Modify the `streamlit_app.py` code to include a `st.checkbox(\"Show raw input data\")`. When checked, display the `input_data` array.\n",
        "\n",
        "-----\n",
        "\n",
        "### **7.2.3 Flask for AI Web APIs**\n",
        "\n",
        "  * **Concept:**\n",
        "    **Flask** is a lightweight Python web framework, often called a \"microframework\" because it provides the bare essentials for web development without imposing a rigid structure. It's highly flexible and commonly used for building web APIs (RESTful APIs) that serve predictions from AI models.\n",
        "\n",
        "      * **Key Features:**\n",
        "\n",
        "          * **Routes:** Define URL paths and the Python functions that handle requests to those paths.\n",
        "          * **Request/Response:** Easily access incoming request data (e.g., JSON payload) and send back structured responses (e.g., JSON predictions).\n",
        "          * **Templating (Jinja2):** (Optional for APIs, but useful for full web apps) Render HTML templates.\n",
        "          * **Extensibility:** A rich ecosystem of extensions for databases, authentication, etc.\n",
        "\n",
        "      * **Pros:**\n",
        "\n",
        "          * High degree of flexibility and control over the application's structure.\n",
        "          * Excellent for building RESTful APIs.\n",
        "          * Lightweight and minimal overhead.\n",
        "          * Good for integrating with other front-end technologies (React, Vue, etc.).\n",
        "\n",
        "      * **Cons:**\n",
        "\n",
        "          * Requires more boilerplate code compared to Streamlit for interactive UIs.\n",
        "          * Requires basic understanding of HTTP methods (GET, POST), JSON, and web concepts.\n",
        "          * Not designed for rapid interactive data exploration out-of-the-box.\n",
        "\n",
        "  * **Colab Example:**\n",
        "    We'll create a simple Flask API that loads our dummy AI model and provides an endpoint for predictions. You can then test this API using `curl` or by visiting the URL in a browser."
      ],
      "metadata": {
        "id": "37EB2Ey2u9j-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Flask pyngrok -q\n",
        "# Save Flask app code to file\n",
        "flask_app_code = \"\"\"\n",
        "from flask import Flask, request, jsonify\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "def load_my_ai_model_flask():\n",
        "    time.sleep(1)  # Simulate model loading\n",
        "    class DummyModel:\n",
        "        def predict(self, input_data):\n",
        "            # Predict 1 if sum > 100, else 0\n",
        "            return np.array([1]) if np.sum(input_data) > 100 else np.array([0])\n",
        "    return DummyModel()\n",
        "\n",
        "FLASK_AI_MODEL = load_my_ai_model_flask()\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return \"Welcome to the Flask AI Model API! Use /predict endpoint.\"\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    if not request.is_json:\n",
        "        return jsonify({\"error\": \"Request must be JSON\"}), 400\n",
        "\n",
        "    data = request.get_json()\n",
        "\n",
        "    if 'features' not in data or not isinstance(data['features'], list):\n",
        "        return jsonify({\"error\": \"Missing 'features' key or 'features' is not a list\"}), 400\n",
        "\n",
        "    try:\n",
        "        input_features = np.array([data['features']], dtype=np.float32)\n",
        "        if input_features.shape[1] != 2:\n",
        "            return jsonify({\"error\": \"Expected 2 features in the input list\"}), 400\n",
        "\n",
        "        prediction = FLASK_AI_MODEL.predict(input_features)\n",
        "        return jsonify({\"prediction\": int(prediction[0])})\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": f\"Error processing request: {str(e)}\"}), 500\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()\n",
        "\"\"\"\n",
        "\n",
        "# Write to file\n",
        "with open(\"flask_app.py\", \"w\") as f:\n",
        "    f.write(flask_app_code)\n",
        "\n",
        "print(\"✅ Flask app saved as 'flask_app.py'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuiCdMvkuk2K",
        "outputId": "b97c2698-c4c2-409a-8e76-edd19d0d8a03"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Flask app saved as 'flask_app.py'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "app = Flask(**name**)\n",
        "\n",
        "# Simulate loading a pre-trained model\n",
        "\n",
        "# Use @lru\\_cache or similar for actual model loading in production\n",
        "\n",
        "# For simplicity, we'll load it once globally for this demo\n",
        "\n",
        "def load\\_my\\_ai\\_model\\_flask\\_internal():\n",
        "time.sleep(1) \\# Simulate loading time\n",
        "class DummyModel:\n",
        "def predict(self, input\\_data):\n",
        "\\# Simple dummy logic: if sum of features \\> 100, predict 1, else 0\n",
        "if np.sum(input\\_data) \\> 100:\n",
        "return np.array([1])\n",
        "else:\n",
        "return np.array([0])\n",
        "return DummyModel()\n",
        "\n",
        "FLASK\\_AI\\_MODEL = load\\_my\\_ai\\_model\\_flask\\_internal()\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "return \"Welcome to the Flask AI Model API\\! Use /predict endpoint with POST request.\"\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "if not request.is\\_json:\n",
        "return jsonify({\"error\": \"Request must be JSON\"}), 400\n",
        "\n",
        "```\n",
        "data = request.get_json()\n",
        "\n",
        "if 'features' not in data or not isinstance(data['features'], list):\n",
        "    return jsonify({\"error\": \"Missing 'features' key or 'features' is not a list\"}), 400\n",
        "\n",
        "try:\n",
        "    input_features = np.array([data['features']], dtype=np.float32)\n",
        "    if input_features.shape[1] != 2:\n",
        "        return jsonify({\"error\": \"Expected 2 features in the input list\"}), 400\n",
        "\n",
        "    prediction = FLASK_AI_MODEL.predict(input_features)\n",
        "\n",
        "    return jsonify({\"prediction\": int(prediction[0])})\n",
        "\n",
        "except Exception as e:\n",
        "    return jsonify({\"error\": f\"Error processing request: {str(e)}\"}), 500\n",
        "```\n",
        "\n",
        "if **name** == '**main**':\n",
        "\\# Run the Flask app\n",
        "app.run(host='0.0.0.0', port=5000)\n",
        "\"\"\"\n",
        "\n",
        "````\n",
        "with open(\"flask_app.py\", \"w\") as f:\n",
        "    f.write(flask_app_code)\n",
        "\n",
        "print(\"\\nFlask app code saved to `flask_app.py`.\")\n",
        "print(\"To run this API in Colab, execute the following in a new cell:\")\n",
        "print(\"!nohup flask run --host=0.0.0.0 --port=5000 &\")\n",
        "print(\"!nohup ngrok http 5000 &\")\n",
        "print(\"!sleep 2\") # Give ngrok a moment to start\n",
        "print(\"!curl -s http://localhost:4040/api/tunnels | grep -o 'https://[^/]*\\\\.ngrok\\\\.io'\")\n",
        "print(\"\\nOnce the ngrok URL is generated, you can test the API using `curl` or Postman:\")\n",
        "print(\"Example `curl` command (replace URL with your ngrok URL):\")\n",
        "print(\"!curl -X POST -H 'Content-Type: application/json' -d '{\\\"features\\\": [60.5, 30.2]}' YOUR_NGROK_URL/predict\")\n",
        "```\n",
        "````\n",
        "\n",
        "  * **Exercise:**\n",
        "    1.  In the Flask example, why is `FLASK_AI_MODEL = load_my_ai_model_flask()` placed outside the `predict()` function? What would be the performance implication if it were inside?\n",
        "    2.  Modify the Flask `predict` endpoint to also accept a `user_id` in the JSON payload and include it in the response (e.g., `{\"prediction\": 0, \"user_id\": \"abc\"}`).\n",
        "\n",
        "-----\n",
        "\n",
        "### **7.2.4 Streamlit vs. Flask: When to Use Which**\n",
        "\n",
        "  * **Concept:**\n",
        "    The choice between Streamlit and Flask (or other frameworks like FastAPI, Django) largely depends on your project's requirements, your team's expertise, and the stage of development.\n",
        "\n",
        "    | Feature / Aspect      | Streamlit                                       | Flask                                                         |\n",
        "    | :-------------------- | :---------------------------------------------- | :------------------------------------------------------------ |\n",
        "    | **Primary Use Case** | Interactive dashboards, demos, rapid prototyping, internal tools, data exploration | REST APIs, microservices, custom web applications, backend for front-ends |\n",
        "    | **Ease of Use** | Very high (Python-only, minimal web dev knowledge) | Moderate (requires basic understanding of web concepts, HTML/CSS for UI) |\n",
        "    | **Development Speed** | Extremely fast for simple UIs                   | Fast for APIs, moderate for full web apps                     |\n",
        "    | **UI Customization** | Limited (uses built-in widgets and layout)      | High (full control over HTML, CSS, JavaScript)                |\n",
        "    | **Backend Control** | Less direct control over HTTP requests/responses, single-threaded by default | Full control over routes, requests, responses, can be multi-threaded/asynchronous |\n",
        "    | **Scalability** | Better for lower traffic, single-instance demos | Highly scalable for production APIs (with Gunicorn/Nginx/Docker) |\n",
        "    | **State Management** | Reactive model (reruns script), `st.session_state`, `@st.cache` | Explicitly managed (e.g., global variables, databases, session objects) |\n",
        "    | **Ideal for Data Scientists** | Yes, especially for quick sharing and visualization | Yes, for building API endpoints for models                    |\n",
        "    | **Ideal for Web Developers** | Less common for traditional web dev           | Yes, good for building robust web services                    |\n",
        "\n",
        "  * **Colab Explanation:**\n",
        "  "
      ],
      "metadata": {
        "id": "Y2-Wg8vQvt46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- 7.2.4 Streamlit vs. Flask: When to Use Which ---\")\n",
        "\n",
        "print(\"\\n**Choose Streamlit if:**\")\n",
        "print(\"- You are a data scientist who wants to quickly build an interactive demo or dashboard.\")\n",
        "print(\"- You need to showcase your AI model's capabilities to non-technical stakeholders.\")\n",
        "print(\"- You don't want to write HTML, CSS, or JavaScript.\")\n",
        "print(\"- Your application is primarily for internal use, a proof-of-concept, or has low traffic.\")\n",
        "print(\"  *Example:* A quick app to visualize a classification model's decision boundary.\")\n",
        "\n",
        "print(\"\\n**Choose Flask if:**\")\n",
        "print(\"- You need to build a robust API endpoint for your AI model that other applications will consume.\")\n",
        "print(\"- You require full control over the web application's structure, routing, and HTTP interactions.\")\n",
        "print(\"- You are comfortable with basic web development concepts (HTTP methods, JSON, potentially HTML/CSS/JS).\")\n",
        "print(\"- Your application needs to scale to handle high traffic or complex backend logic.\")\n",
        "print(\"  *Example:* An API for a mobile app to send an image and get an object detection result.\")\n",
        "\n",
        "print(\"\\nBoth are powerful tools, but they cater to different needs in the AI deployment lifecycle.\")\n",
        "print(\"Often, data scientists use Streamlit for initial exploration and demos, and then transition to Flask (or FastAPI) for production-grade APIs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqPTn-9ivMU2",
        "outputId": "8927af71-e196-41b0-c96d-5acf81961f9f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 7.2.4 Streamlit vs. Flask: When to Use Which ---\n",
            "\n",
            "**Choose Streamlit if:**\n",
            "- You are a data scientist who wants to quickly build an interactive demo or dashboard.\n",
            "- You need to showcase your AI model's capabilities to non-technical stakeholders.\n",
            "- You don't want to write HTML, CSS, or JavaScript.\n",
            "- Your application is primarily for internal use, a proof-of-concept, or has low traffic.\n",
            "  *Example:* A quick app to visualize a classification model's decision boundary.\n",
            "\n",
            "**Choose Flask if:**\n",
            "- You need to build a robust API endpoint for your AI model that other applications will consume.\n",
            "- You require full control over the web application's structure, routing, and HTTP interactions.\n",
            "- You are comfortable with basic web development concepts (HTTP methods, JSON, potentially HTML/CSS/JS).\n",
            "- Your application needs to scale to handle high traffic or complex backend logic.\n",
            "  *Example:* An API for a mobile app to send an image and get an object detection result.\n",
            "\n",
            "Both are powerful tools, but they cater to different needs in the AI deployment lifecycle.\n",
            "Often, data scientists use Streamlit for initial exploration and demos, and then transition to Flask (or FastAPI) for production-grade APIs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Exercise:**\n",
        "\n",
        "    1.  You are a data scientist who has developed a new algorithm for predicting stock prices. You want to quickly build a web interface where you can input stock features (e.g., historical prices, trading volume) and see the predicted price, along with some interactive charts. Which framework would you choose, and why?\n",
        "    2.  Your company needs to integrate a sentiment analysis model into its customer support system. The support system is written in Java and needs to send customer chat messages to your model and receive a sentiment score. Which framework would you choose to deploy your Python-based sentiment model, and why?\n"
      ],
      "metadata": {
        "id": "GBPeFpt1v-lT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.3: Hosting on Hugging Face, Render, or Vercel**\n"
      ],
      "metadata": {
        "id": "VMYzVQfIwWId"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7.3.1 Cloud Hosting for AI Models & Web Apps**\n",
        "\n",
        "  * **Concept:**\n",
        "    Cloud hosting refers to running your applications (including AI models wrapped in web apps or APIs) on remote servers managed by a cloud provider, rather than on your local machine.\n",
        "\n",
        "      * **Why Cloud Hosting?**\n",
        "\n",
        "          * **Global Accessibility:** Your application is available 24/7 from anywhere with an internet connection.\n",
        "          * **Scalability:** Cloud platforms can automatically (or manually) adjust resources (CPU, RAM, GPU) to handle varying user loads, from a few users to millions.\n",
        "          * **Reliability & Uptime:** Providers offer high availability, backups, and disaster recovery, ensuring your application is almost always online.\n",
        "          * **Maintenance & Security:** The cloud provider handles infrastructure maintenance, security updates, and underlying hardware, freeing you to focus on your AI model and application logic.\n",
        "          * **Cost-Effectiveness:** Pay-as-you-go models mean you only pay for the resources you consume, often more efficient than managing your own servers.\n",
        "\n",
        "      * **Types of Cloud Services (Simplified):**\n",
        "\n",
        "          * **Platform as a Service (PaaS):** Provides a complete environment (operating system, web server, database, etc.) for you to deploy your code without managing the underlying infrastructure. Examples: Render, Heroku.\n",
        "          * **Serverless Functions (Function as a Service - FaaS):** You deploy individual functions, and the cloud provider runs them on demand, scaling automatically. You only pay when your function is executed. Examples: AWS Lambda, Google Cloud Functions, Vercel Serverless Functions.\n",
        "\n",
        "  * **Colab Explanation:**"
      ],
      "metadata": {
        "id": "tgoorfjywfPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- 7.3.1 Cloud Hosting for AI Models & Web Apps ---\")\n",
        "\n",
        "print(\"Cloud hosting is essential for making your AI models available to a wider audience or integrating them into other systems.\")\n",
        "print(\"It moves your application from your local machine to powerful, managed servers in the cloud.\")\n",
        "\n",
        "print(\"\\nBenefits of Cloud Hosting:\")\n",
        "print(\"- **Always On:** Your app is available 24/7.\")\n",
        "print(\"- **Scales Automatically:** Handles more users without manual intervention.\")\n",
        "print(\"- **Less Maintenance:** The cloud provider manages the servers.\")\n",
        "print(\"- **Global Reach:** Accessible from anywhere in the world.\")\n",
        "\n",
        "print(\"\\nWe'll look at three popular platforms for deploying Python AI applications:\")\n",
        "print(\"1. Hugging Face Spaces: Great for quick ML demos (especially Streamlit/Gradio).\")\n",
        "print(\"2. Render: A versatile platform for web services and APIs (good for Flask/FastAPI).\")\n",
        "print(\"3. Vercel: Excellent for static sites and serverless functions (can host Python APIs).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_tUb5fpv5rh",
        "outputId": "0f4e3078-4cfd-4e1a-e59f-4b17a39907a4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 7.3.1 Cloud Hosting for AI Models & Web Apps ---\n",
            "Cloud hosting is essential for making your AI models available to a wider audience or integrating them into other systems.\n",
            "It moves your application from your local machine to powerful, managed servers in the cloud.\n",
            "\n",
            "Benefits of Cloud Hosting:\n",
            "- **Always On:** Your app is available 24/7.\n",
            "- **Scales Automatically:** Handles more users without manual intervention.\n",
            "- **Less Maintenance:** The cloud provider manages the servers.\n",
            "- **Global Reach:** Accessible from anywhere in the world.\n",
            "\n",
            "We'll look at three popular platforms for deploying Python AI applications:\n",
            "1. Hugging Face Spaces: Great for quick ML demos (especially Streamlit/Gradio).\n",
            "2. Render: A versatile platform for web services and APIs (good for Flask/FastAPI).\n",
            "3. Vercel: Excellent for static sites and serverless functions (can host Python APIs).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Exercise:**\n",
        "\n",
        "    1.  You have developed a machine learning model that predicts housing prices. Your goal is to allow real estate agents to use this model by entering property details into a web form. Why would hosting this model in the cloud be preferable to running it on your personal computer?\n",
        "    2.  What is the main difference between a \"Platform as a Service (PaaS)\" and \"Serverless Functions\" in the context of deploying a simple AI prediction API?\n",
        "\n",
        "-----\n",
        "\n",
        "### **7.3.2 Hugging Face Spaces (for ML Demos & Models)**\n",
        "\n",
        "  * **Concept:**\n",
        "    Hugging Face Spaces is a platform specifically designed for hosting machine learning demos, models, and datasets. It integrates seamlessly with popular ML UI frameworks like Streamlit and Gradio, allowing data scientists to quickly deploy interactive applications directly from their code.\n",
        "\n",
        "      * **Key Features:**\n",
        "\n",
        "          * **Easy Deployment:** Connects directly to a Git repository (e.g., GitHub) and automatically builds/deploys your app.\n",
        "          * **Framework Agnostic (but favors Streamlit/Gradio):** While you can use custom Dockerfiles, it has native support for Streamlit and Gradio.\n",
        "          * **Integrated Ecosystem:** Part of the larger Hugging Face ecosystem, making it easy to use models from the Hugging Face Hub.\n",
        "          * **Free Tier:** Offers a generous free tier for personal projects and demos.\n",
        "          * **Community & Sharing:** Encourages sharing and collaboration within the ML community.\n",
        "\n",
        "      * **Pros:**\n",
        "\n",
        "          * Extremely fast for deploying interactive ML demos.\n",
        "          * No need for complex web development setup.\n",
        "          * Ideal for showcasing models and research.\n",
        "          * Version control integrated via Git.\n",
        "\n",
        "      * **Cons:**\n",
        "\n",
        "          * Not a general-purpose web hosting platform (e.g., not for complex e-commerce sites).\n",
        "          * Resource limits on the free tier (CPU, RAM, disk space).\n",
        "          * Less control over the underlying infrastructure compared to IaaS (Infrastructure as a Service).\n",
        "\n",
        "  * **Colab Example (Conceptual Setup):**\n",
        "    We can't directly deploy to Hugging Face Spaces from Colab, but we can prepare the necessary files and understand the folder structure."
      ],
      "metadata": {
        "id": "kYFW4OW7wt4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- 7.3.2 Hugging Face Spaces (for ML Demos & Models) ---\")\n",
        "\n",
        "print(\"Hugging Face Spaces is like a playground for ML demos. It's super easy to get your Streamlit or Gradio app online.\")\n",
        "\n",
        "# --- Conceptual File Structure for a Hugging Face Space ---\n",
        "# Your Hugging Face Space will typically be a Git repository.\n",
        "# Here's what the root of that repository would look like for a Streamlit app:\n",
        "\n",
        "# 1. `app.py` (Your Streamlit or Gradio application code)\n",
        "streamlit_app_code_hf = \"\"\""
      ],
      "metadata": {
        "id": "w6uW8H1jwqBW",
        "outputId": "f46f982a-ec00-4647-830f-a741c03cee05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-1757159545.py, line 10)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1757159545.py\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    streamlit_app_code_hf = \"\"\"\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import streamlit as st\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Simulate loading a small AI model\n",
        "\n",
        "@st.cache\\_resource\n",
        "def load\\_simple\\_classifier():\n",
        "time.sleep(1) \\# Simulate loading time\n",
        "class SimpleClassifier:\n",
        "def predict(self, input\\_value):\n",
        "if input\\_value \\> 0.5:\n",
        "return \"Positive\"\n",
        "else:\n",
        "return \"Negative\"\n",
        "return SimpleClassifier()\n",
        "\n",
        "classifier = load\\_simple\\_classifier()\n",
        "\n",
        "st.set\\_page\\_config(page\\_title=\"HF Spaces Demo\", layout=\"centered\")\n",
        "st.title(\"Hugging Face Spaces: Simple Classifier Demo\")\n",
        "st.write(\"Enter a value between 0 and 1 to get a classification.\")\n",
        "\n",
        "value = st.slider(\"Input Value\", 0.0, 1.0, 0.5)\n",
        "\n",
        "if st.button(\"Classify\"):\n",
        "prediction = classifier.predict(value)\n",
        "st.success(f\"The model predicts: **{prediction}**\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.write(\"This app is hosted on Hugging Face Spaces\\!\")\n",
        "\"\"\"\n",
        "\\# Save this to a dummy file to show structure\n",
        "with open(\"hf\\_space\\_app.py\", \"w\") as f:\n",
        "f.write(streamlit\\_app\\_code\\_hf)\n",
        "print(\"\\\\n`hf_space_app.py` (your Streamlit app) created.\")\n",
        "\n",
        "```\n",
        "# 2. `requirements.txt` (List of Python dependencies)\n",
        "requirements_hf = \"\"\"\n",
        "```\n",
        "\n",
        "streamlit\n",
        "numpy\n",
        "tensorflow \\# if you use a TF model\n",
        "\n",
        "# Add any other libraries your app needs (e.g., scikit-learn, pandas)\n",
        "\n",
        "```\n",
        "\"\"\"\n",
        "with open(\"hf_space_requirements.txt\", \"w\") as f:\n",
        "    f.write(requirements_hf)\n",
        "print(\"`hf_space_requirements.txt` created.\")\n",
        "\n",
        "\n",
        "# 3. `README.md` (Optional, but good for description)\n",
        "readme_hf = \"\"\"\n",
        "```\n",
        "\n",
        "-----\n",
        "\n",
        "## title: My Simple Classifier emoji: 🚀 colorFrom: blue colorTo: purple sdk: streamlit sdk\\_version: 1.20.0 app\\_file: hf\\_space\\_app.py pinned: false\n",
        "\n",
        "# My Simple Classifier Demo\n",
        "\n",
        "This is a basic Streamlit application deployed on Hugging Face Spaces.\n",
        "It demonstrates a simple classification model.\n",
        "\"\"\"\n",
        "with open(\"hf\\_space\\_README.md\", \"w\") as f:\n",
        "f.write(readme\\_hf)\n",
        "print(\"`hf_space_README.md` created (for Space configuration).\")\n",
        "\n",
        "````\n",
        "print(\"\\nTo deploy to Hugging Face Spaces:\")\n",
        "print(\"1. Create a new Space on Hugging Face (https://huggingface.co/spaces/new).\")\n",
        "print(\"2. Choose 'Streamlit' or 'Gradio' SDK.\")\n",
        "print(\"3. Connect your Git repository (e.g., GitHub) or upload these files directly.\")\n",
        "print(\"4. Push `hf_space_app.py`, `hf_space_requirements.txt`, and `hf_space_README.md` to your Space's Git repo.\")\n",
        "print(\"Hugging Face will automatically build and deploy your app!\")\n",
        "```\n",
        "````\n",
        "\n",
        "  * **Exercise:**\n",
        "    1.  You have a Python script that uses `matplotlib` to generate a complex data visualization. You want to share this interactive visualization with your colleagues using Hugging Face Spaces. What two main files would you need in your Git repository for the Space?\n",
        "    2.  What is the primary benefit of using Hugging Face Spaces for a data scientist who wants to quickly showcase a new AI model, compared to setting up a traditional web server?\n",
        "\n",
        "-----\n",
        "\n",
        "### **7.3.3 Render (for Web Services & APIs)**\n",
        "\n",
        "  * **Concept:**\n",
        "    **Render** is a unified cloud platform that allows you to host web applications, APIs, static sites, databases, and more, directly from your Git repository. It's a Platform-as-a-Service (PaaS) that abstracts away much of the infrastructure complexity, making it a good choice for deploying Flask or FastAPI-based AI APIs.\n",
        "\n",
        "      * **Key Features:**\n",
        "\n",
        "          * **Git-based Deployment:** Automatically deploys new versions whenever you push changes to your Git repository.\n",
        "          * **Multiple Service Types:** Supports web services (for Flask/FastAPI), static sites, background workers, databases, etc.\n",
        "          * **Auto-scaling:** Can automatically scale your web services up or down based on traffic.\n",
        "          * **Custom Domains & SSL:** Easy to connect your own domain and get free SSL certificates.\n",
        "          * **Environment Variables:** Securely manage API keys and other sensitive information.\n",
        "          * **Build & Runtime Logs:** Provides detailed logs for debugging.\n",
        "\n",
        "      * **Pros:**\n",
        "\n",
        "          * Versatile for various backend services, not just ML demos.\n",
        "          * Good for production-ready APIs with more control than Streamlit Cloud.\n",
        "          * Simplified deployment compared to raw cloud VMs.\n",
        "          * Generous free tier for static sites, and a free tier for web services (with some limitations like spin-down).\n",
        "\n",
        "      * **Cons:**\n",
        "\n",
        "          * Can be more complex to configure than Hugging Face Spaces for simple demos.\n",
        "          * Free tier web services spin down after inactivity, leading to cold starts.\n",
        "          * Less focused on ML-specific features compared to Hugging Face.\n",
        "\n",
        "  * **Colab Example (Conceptual Setup):**\n",
        "    We'll prepare the files for a simple Flask API that could be deployed on Render."
      ],
      "metadata": {
        "id": "ix28FZYOw8Bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- 7.3.3 Render (for Web Services & APIs) ---\")\n",
        "\n",
        "print(\"Render is a great PaaS for deploying Flask or FastAPI APIs. It handles the server setup for you.\")\n",
        "\n",
        "# --- Conceptual File Structure for a Render Web Service ---\n",
        "# Your Render project will typically be a Git repository.\n",
        "# Here's what the root of that repository would look like for a Flask API:\n",
        "\n",
        "# 1. `app.py` (Your Flask API code)\n",
        "flask_api_code_render = \"\"\""
      ],
      "metadata": {
        "id": "YUY6AaRMw22l",
        "outputId": "19b2365b-d99f-446d-f2ef-25519b272650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-282051235.py, line 10)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-282051235.py\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    flask_api_code_render = \"\"\"\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "app = Flask(**name**)\n",
        "\n",
        "# Simulate loading a model (should be done once globally)\n",
        "\n",
        "# In a real app, you'd load your actual TensorFlow/Keras model here.\n",
        "\n",
        "# Use @lru\\_cache for more robust caching if needed.\n",
        "\n",
        "def load\\_model\\_for\\_render():\n",
        "time.sleep(2) \\# Simulate heavy model loading\n",
        "class RenderDummyModel:\n",
        "def predict(self, data):\n",
        "\\# Simple logic: if input is even, predict 'A', else 'B'\n",
        "if len(data) \\> 0 and data[0] % 2 == 0:\n",
        "return \"A\"\n",
        "else:\n",
        "return \"B\"\n",
        "return RenderDummyModel()\n",
        "\n",
        "# Load the model when the app starts\n",
        "\n",
        "RENDER\\_MODEL = load\\_model\\_for\\_render()\n",
        "print(\"Render Flask app: Dummy model loaded.\")\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "return \"Render Flask AI API is running\\! Use /predict endpoint.\"\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "if not request.is\\_json:\n",
        "return jsonify({\"error\": \"Request must be JSON\"}), 400\n",
        "\n",
        "```\n",
        "data = request.get_json()\n",
        "input_value = data.get('value')\n",
        "\n",
        "if input_value is None or not isinstance(input_value, (int, float)):\n",
        "    return jsonify({\"error\": \"Please provide a 'value' (integer or float) in the JSON body.\"}), 400\n",
        "\n",
        "try:\n",
        "    prediction = RENDER_MODEL.predict(input_value)\n",
        "    return jsonify({\"input_value\": input_value, \"prediction\": prediction})\n",
        "except Exception as e:\n",
        "    return jsonify({\"error\": f\"Prediction failed: {str(e)}\"}), 500\n",
        "```\n",
        "\n",
        "if **name** == '**main**':\n",
        "\\# Render will use Gunicorn or a similar WSGI server in production,\n",
        "\\# so this **main** block is mostly for local testing.\n",
        "app.run(host='0.0.0.0', port=os.environ.get('PORT', 5000))\n",
        "\"\"\"\n",
        "with open(\"render\\_app.py\", \"w\") as f:\n",
        "f.write(flask\\_api\\_code\\_render)\n",
        "print(\"\\\\n`render_app.py` (your Flask API) created.\")\n",
        "\n",
        "```\n",
        "# 2. `requirements.txt` (List of Python dependencies)\n",
        "requirements_render = \"\"\"\n",
        "```\n",
        "\n",
        "Flask\n",
        "numpy\n",
        "gunicorn \\# Recommended for production WSGI server\n",
        "\n",
        "# Add any other libraries your API needs (e.g., tensorflow, scikit-learn)\n",
        "\n",
        "```\n",
        "\"\"\"\n",
        "with open(\"render_requirements.txt\", \"w\") as f:\n",
        "    f.write(requirements_render)\n",
        "print(\"`render_requirements.txt` created.\")\n",
        "\n",
        "\n",
        "# 3. `render.yaml` (Optional, for more complex configurations)\n",
        "# This file tells Render how to build and deploy your service.\n",
        "# For a simple Flask app, Render can often auto-detect, but it's good practice.\n",
        "render_yaml_config = \"\"\"\n",
        "```\n",
        "\n",
        "# render.yaml\n",
        "\n",
        "# This file tells Render how to build and deploy your service.\n",
        "\n",
        "services:\n",
        "\n",
        "  - type: web\n",
        "    name: my-flask-ai-api\n",
        "    env: python\n",
        "    buildCommand: \"pip install -r render\\_requirements.txt\"\n",
        "    startCommand: \"gunicorn render\\_app:app\" \\# Use gunicorn to run your Flask app\n",
        "    envVars:\n",
        "\n",
        "      - key: PYTHON\\_VERSION\n",
        "        value: 3.9.12 \\# Specify Python version (optional but good practice)\n",
        "        \"\"\"\n",
        "        with open(\"render.yaml\", \"w\") as f:\n",
        "        f.write(render\\_yaml\\_config)\n",
        "        print(\"`render.yaml` (Render configuration) created.\")\n",
        "\n",
        "    print(\"\\\\nTo deploy to Render:\")\n",
        "    print(\"1. Create a Render account (https://render.com/).\")\n",
        "    print(\"2. Create a new 'Web Service'.\")\n",
        "    print(\"3. Connect your Git repository (e.g., GitHub) containing these files.\")\n",
        "    print(\"4. Render will detect the `render.yaml` or you can manually configure build/start commands.\")\n",
        "    print(\"5. Deploy the service. Render will provide a public URL for your API.\")\n",
        "\n",
        "    ```\n",
        "    \n",
        "    ```\n",
        "\n",
        "<!-- end list -->\n",
        "\n",
        "  * **Exercise:**\n",
        "    1.  What is the purpose of `gunicorn` in the `startCommand` for a Flask application deployed on Render?\n",
        "    2.  If your Flask API needs to access a sensitive API key for an external service, how would you typically manage this securely when deploying to Render?\n",
        "\n",
        "-----\n",
        "\n",
        "### **7.3.4 Vercel (for Frontend & Serverless Functions)**\n",
        "\n",
        "  * **Concept:**\n",
        "    **Vercel** is a cloud platform for frontend developers, best known for hosting static sites and serverless functions. While it's primarily associated with JavaScript frameworks like Next.js and React, it also supports Python for serverless functions, making it possible to deploy lightweight AI APIs.\n",
        "\n",
        "      * **Key Features:**\n",
        "\n",
        "          * **Zero Configuration:** Often deploys directly from Git with minimal setup.\n",
        "          * **Serverless Functions:** Run backend code (including Python) on demand, scaling automatically to zero when idle.\n",
        "          * **Global Edge Network (CDN):** Fast content delivery worldwide.\n",
        "          * **Automatic SSL & CI/CD:** Built-in SSL and continuous deployment from Git.\n",
        "          * **Generous Free Tier:** Excellent for static sites and serverless functions (with limits on execution time, memory, etc.).\n",
        "\n",
        "      * **Pros:**\n",
        "\n",
        "          * Incredibly fast deployment and updates.\n",
        "          * Generous free tier for serverless functions.\n",
        "          * Excellent developer experience (DX).\n",
        "          * Ideal for small, stateless AI inference tasks that don't require heavy computation or long cold starts.\n",
        "\n",
        "      * **Cons:**\n",
        "\n",
        "          * Python serverless functions can experience \"cold starts\" (initial delay when function is invoked after inactivity).\n",
        "          * Not suitable for large AI models that require significant memory or GPU, or long-running processes.\n",
        "          * Primarily frontend-focused; more complex for full-stack Python applications.\n",
        "          * Stateless nature means model loading needs careful handling (e.g., loading outside handler or using specific serverless patterns).\n",
        "\n",
        "  * **Colab Example (Conceptual Setup):**\n",
        "    We'll prepare the files for a simple Python serverless function that Vercel can deploy."
      ],
      "metadata": {
        "id": "rg0DnA52xH6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- 7.3.4 Vercel (for Frontend & Serverless Functions) ---\")\n",
        "\n",
        "print(\"Vercel is great for fast frontends and serverless functions. You can use it to host lightweight Python AI APIs.\")\n",
        "\n",
        "# --- Conceptual File Structure for a Vercel Serverless Function ---\n",
        "# Vercel looks for an `api` directory at the root of your Git repository.\n",
        "# Inside `api`, each Python file becomes a serverless function.\n",
        "\n",
        "# 1. `api/predict.py` (Your Python Serverless Function)\n",
        "# Vercel expects a `handler` function or a Flask/FastAPI app object.\n",
        "# For simplicity, we'll use a basic Flask app as Vercel can detect it.\n",
        "vercel_api_code = \"\"\""
      ],
      "metadata": {
        "id": "4zdCIP0gxDzf",
        "outputId": "73ee97d4-f20f-4da5-8b1a-848a457ac05a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-450912643.py, line 12)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-450912643.py\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    vercel_api_code = \"\"\"\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "app = Flask(**name**)\n",
        "\n",
        "# Model loading outside the request handler to minimize cold start impact\n",
        "\n",
        "# This code runs once when the serverless function \"wakes up\"\n",
        "\n",
        "def load\\_model\\_for\\_vercel():\n",
        "time.sleep(1) \\# Simulate model loading\n",
        "class VercelDummyModel:\n",
        "def predict(self, input\\_data):\n",
        "\\# Simple logic: if input is greater than 50, predict 'High', else 'Low'\n",
        "if input\\_data \\> 50:\n",
        "return \"High\"\n",
        "else:\n",
        "return \"Low\"\n",
        "return VercelDummyModel()\n",
        "\n",
        "VERCEL\\_MODEL = load\\_model\\_for\\_vercel()\n",
        "print(\"Vercel Serverless Function: Dummy model loaded.\") \\# This will appear in Vercel logs\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "if not request.is\\_json:\n",
        "return jsonify({\"error\": \"Request must be JSON\"}), 400\n",
        "\n",
        "```\n",
        "data = request.get_json()\n",
        "input_value = data.get('value')\n",
        "\n",
        "if input_value is None or not isinstance(input_value, (int, float)):\n",
        "    return jsonify({\"error\": \"Please provide a 'value' (integer or float) in the JSON body.\"}), 400\n",
        "\n",
        "try:\n",
        "    prediction = VERCEL_MODEL.predict(input_value)\n",
        "    return jsonify({\"input_value\": input_value, \"prediction\": prediction})\n",
        "except Exception as e:\n",
        "    return jsonify({\"error\": f\"Prediction failed: {str(e)}\"}), 500\n",
        "```\n",
        "\n",
        "# Vercel will automatically detect this Flask app object and serve it.\n",
        "\n",
        "# No need for app.run() in serverless context.\n",
        "\n",
        "```\n",
        "\"\"\"\n",
        "# Create the `api` directory\n",
        "os.makedirs(\"api\", exist_ok=True)\n",
        "with open(\"api/predict.py\", \"w\") as f:\n",
        "    f.write(vercel_api_code)\n",
        "print(\"\\n`api/predict.py` (your Python Serverless Function) created.\")\n",
        "\n",
        "\n",
        "# 2. `requirements.txt` (List of Python dependencies for serverless functions)\n",
        "# This should be in the same directory as the function (e.g., `api/requirements.txt`)\n",
        "vercel_requirements = \"\"\"\n",
        "```\n",
        "\n",
        "Flask\n",
        "numpy\n",
        "\n",
        "# Add any other libraries your function needs (e.g., tensorflow, scikit-learn)\n",
        "\n",
        "```\n",
        "\"\"\"\n",
        "with open(\"api/requirements.txt\", \"w\") as f:\n",
        "    f.write(vercel_requirements)\n",
        "print(\"`api/requirements.txt` created.\")\n",
        "\n",
        "\n",
        "# 3. `vercel.json` (Optional, for Vercel project configuration)\n",
        "# This file goes in the root of your repository.\n",
        "vercel_config = \"\"\"\n",
        "```\n",
        "\n",
        "{\n",
        "\"version\": 2,\n",
        "\"builds\": [\n",
        "{\n",
        "\"src\": \"api/*.py\",\n",
        "\"use\": \"@vercel/python\"\n",
        "}\n",
        "],\n",
        "\"routes\": [\n",
        "{\n",
        "\"src\": \"/api/(.*)\",\n",
        "\"dest\": \"/api/$1\"\n",
        "}\n",
        "]\n",
        "}\n",
        "\"\"\"\n",
        "with open(\"vercel.json\", \"w\") as f:\n",
        "f.write(vercel\\_config)\n",
        "print(\"`vercel.json` (Vercel project configuration) created.\")\n",
        "\n",
        "````\n",
        "print(\"\\nTo deploy to Vercel:\")\n",
        "print(\"1. Create a Vercel account (https://vercel.com/).\")\n",
        "print(\"2. Create a new project and connect your Git repository (e.g., GitHub) containing these files.\")\n",
        "print(\"3. Vercel will automatically detect the `api` directory and deploy your serverless functions.\")\n",
        "print(\"4. Access your function at `YOUR_VERCEL_URL/api/predict` (for POST requests).\")\n",
        "```\n",
        "````\n",
        "\n",
        "  * **Exercise:**\n",
        "    1.  In the Vercel example, why is the `VERCEL_MODEL = load_model_for_vercel()` line placed outside the `predict()` function, and what is the benefit of this placement in a serverless environment?\n",
        "    2.  What is a \"cold start\" in the context of serverless functions, and why might it be a concern for AI models deployed on Vercel?\n",
        "\n",
        "-----\n",
        "\n",
        "### **7.3.5 Choosing the Right Platform**\n",
        "\n",
        "  * **Concept:**\n",
        "    The best platform depends on your specific needs:\n",
        "\n",
        "      * **Hugging Face Spaces:**\n",
        "\n",
        "          * **Best for:** Rapid prototyping, interactive ML demos, showcasing models, sharing within the ML community.\n",
        "          * **Strengths:** Simplest setup for Streamlit/Gradio, integrated with Hugging Face Hub, free.\n",
        "          * **Limitations:** Not for complex web apps, resource constraints for heavy models/traffic.\n",
        "\n",
        "      * **Render:**\n",
        "\n",
        "          * **Best for:** Production-ready web services and APIs (Flask, FastAPI), full-stack applications, background jobs.\n",
        "          * **Strengths:** Easy Git-based deployment, auto-scaling, custom domains, good for general backend services.\n",
        "          * **Limitations:** Free tier web services spin down, might be overkill for very simple demos.\n",
        "\n",
        "      * **Vercel:**\n",
        "\n",
        "          * **Best for:** Static sites, front-end heavy applications with lightweight backend APIs (via serverless functions).\n",
        "          * **Strengths:** Incredible developer experience, fast global CDN, generous free tier for serverless.\n",
        "          * **Limitations:** Python serverless functions can have cold starts, not suitable for large, memory-intensive AI models or long-running computations.\n",
        "\n",
        "  * **Summary Table:**\n",
        "\n",
        "    | Feature / Aspect       | Hugging Face Spaces                 | Render                                | Vercel                                  |\n",
        "    | :--------------------- | :---------------------------------- | :------------------------------------ | :-------------------------------------- |\n",
        "    | **Primary Use** | ML Demos, Prototypes                | Web Services, APIs, Full-stack Apps   | Static Sites, Frontend + Serverless APIs |\n",
        "    | **Ease of ML Demo** | Very High (Streamlit/Gradio native) | Moderate (requires Flask/FastAPI setup) | Moderate (Python serverless functions)  |\n",
        "    | **Backend Flexibility**| Limited (focused on UI frameworks)  | High (any Python web framework)       | Moderate (serverless functions only)    |\n",
        "    | **Scalability** | Good for demos, limited for high traffic | High (auto-scaling)                   | High (serverless, scales to zero)       |\n",
        "    | **Cost (Free Tier)** | Generous Free Tier                  | Generous Free Tier (web services spin down) | Generous Free Tier (serverless)         |\n",
        "    | **Ideal Model Size** | Small to Medium                     | Any (depending on paid plan)           | Small (due to serverless limits)        |\n",
        "    | **Cold Starts** | Possible                            | Yes (on free tier web services)       | Yes (for serverless functions)          |\n",
        "    | **Target Audience** | Data Scientists, ML Researchers     | Developers, Startups                  | Frontend Developers                     |\n",
        "\n",
        "  * **Colab Explanation:**"
      ],
      "metadata": {
        "id": "zdnFgT_AxSUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- 7.3.5 Choosing the Right Platform ---\")\n",
        "\n",
        "print(\"The best platform for your AI model deployment depends on your specific needs:\")\n",
        "\n",
        "print(\"\\nIf you want to quickly share an interactive ML demo or research prototype:\")\n",
        "print(\"-> **Hugging Face Spaces** is likely your best bet.\")\n",
        "\n",
        "print(\"\\nIf you need a robust, scalable API for your AI model to be consumed by other applications or a custom frontend:\")\n",
        "print(\"-> **Render** is a strong contender.\")\n",
        "\n",
        "print(\"\\nIf you are building a modern web application with a strong frontend and need lightweight backend logic for AI inference:\")\n",
        "print(\"-> **Vercel** with Python Serverless Functions can be a powerful choice.\")\n",
        "\n",
        "print(\"\\nConsider your model's size, traffic expectations, development time, and your team's expertise when making your decision.\")"
      ],
      "metadata": {
        "id": "Hfvx5R-0xOEu",
        "outputId": "7e749e69-9a20-4e56-be8b-b0139ba11392",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 7.3.5 Choosing the Right Platform ---\n",
            "The best platform for your AI model deployment depends on your specific needs:\n",
            "\n",
            "If you want to quickly share an interactive ML demo or research prototype:\n",
            "-> **Hugging Face Spaces** is likely your best bet.\n",
            "\n",
            "If you need a robust, scalable API for your AI model to be consumed by other applications or a custom frontend:\n",
            "-> **Render** is a strong contender.\n",
            "\n",
            "If you are building a modern web application with a strong frontend and need lightweight backend logic for AI inference:\n",
            "-> **Vercel** with Python Serverless Functions can be a powerful choice.\n",
            "\n",
            "Consider your model's size, traffic expectations, development time, and your team's expertise when making your decision.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Exercise:**\n",
        "\n",
        "    1.  You have a large language model (LLM) that requires 16GB of RAM to run inference. You want to host this model as an API. Which of the three platforms (Hugging Face Spaces, Render, Vercel) would be the *least* suitable for this, especially on a free tier, and why?\n",
        "    2.  Your team is developing a new feature for your e-commerce website where users can upload product images, and an AI model will automatically tag them with categories. The website's frontend is built with Next.js. Which hosting platform would offer the most seamless integration for this scenario, and why?\n"
      ],
      "metadata": {
        "id": "HSKAjoL_xfWO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iBp6jyoIxZ9v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}